# ğŸ§  GAttANet-Replication â€“ Global Attention Agreement for CNNs

This repository provides a **PyTorch-based replication** of  
**GAttANet: Global attention agreement for
convolutional neural networks**.

Focus: **implementing and understanding global attention agreement mechanisms** practically,  
without chasing SOTA benchmark results.

- Captures **layer-wide consensus for feature activations** ğŸ”  
- Modular & **plug-and-play with any CNN backbone** ğŸ”—  
- Lightweight & interpretable, ideal for **experimentation and analysis** ğŸ§©  

**Paper reference:** [VanRullen & Alamia, 2021](https://arxiv.org/abs/2104.05575) ğŸ“„

---

## ğŸŒŒ Overview â€“ GAttANet Pipeline

![GAttANet Overview](images/figmix.jpg)


> Each part of the network â€œvotesâ€ on whatâ€™s important.  
> A **global attention query** represents the networkâ€™s overall opinion.  
> Individual features are then **amplified or suppressed** depending on how well they agree with this global consensusâ€”like how humans revise their opinion when comparing with the group.

High-level procedure:

1. Extract feature maps from selected CNN layers (convolutional & dense).  
2. Each spatial position (or neuron) produces a **key & query** vector:  
   $$k_i, q_i = \text{Linear/Conv projection of layer activations}$$
3. Compute **global query** by averaging all queries across layers:  
   $$q_\text{avg} = \frac{1}{N} \sum_\text{layers} \text{mean over space/units}(q)$$
4. Compute **agreement score** for each feature with the global query:  
   $$gatta_i = k_i \cdot q_\text{avg}$$
5. **Modulate activations** based on agreement:  
   $$x_i' = x_i \cdot (1 + \alpha_i \cdot gatta_i)$$

This simple iterative mechanism lets the network highlight features aligned with the overall representation while damping conflicting ones.

---

## ğŸ§® Math Essentials

For convolutional layers:

$$
\begin{aligned}
k_i(x,y,:) &= \sum_c \text{Conv}_i(x,y,c) \cdot K_i(c,:) \\
q_i(x,y,:) &= \sum_c \text{Conv}_i(x,y,c) \cdot Q_i(c,:)
\end{aligned}
$$

For dense layers:

$$
\begin{aligned}
k_j(c,:) &= \text{Dense}_j(c) \cdot K_j(c,:) \\
q_j(c,:) &= \text{Dense}_j(c) \cdot Q_j(c,:)
\end{aligned}
$$

Global query pooling:

$$
q_\text{avg}(m) = \frac{1}{n_c + n_d} \left( \sum_{i \in C} \frac{1}{H_i W_i} \sum_{x,y} q_i(x,y,m) + \sum_{j \in D} \frac{1}{C_j} \sum_c q_j(c,m) \right)
$$

Agreement score:

$$
gatta_i(x,y) = k_i(x,y,:) \cdot q_\text{avg}, \quad
gatta_j(c) = k_j(c,:) \cdot q_\text{avg}
$$

Modulation:

$$
x_i' = x_i \cdot (1 + \alpha_i \cdot gatta_i), \quad
x_j' = x_j \cdot (1 + \alpha_j \cdot gatta_j)
$$

Where:  
- $K_i, Q_i$ = learnable projections  
- $\alpha_i$ = learned scalar controlling attention strength  
- $C$ / $D$ = sets of conv and dense layers  
- $x_i$ = feature map activation  

---

## ğŸ§  Analogy to Human Attention

Imagine the network as a group of people:

- Each **neuron/layer â€œspeaks its mindâ€** (key & query)  
- The network forms a **collective opinion** (global query)  
- Individuals **amplify or suppress their contribution** depending on agreement  
- This mimics **iterative human decision-making**: revise if your idea disagrees with the group  

---

## ğŸ“¦ Repository Structure

```bash
GAttANet-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py          # Conv2d wrapper (1x1, 3x3)
â”‚   â”‚   â”œâ”€â”€ linear_layer.py        # Dense / Linear wrapper
â”‚   â”‚   â””â”€â”€ normalization.py       # optional: LayerNorm for gatta
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”œâ”€â”€ kq_projection.py       # Key / Query projections (Eq.1-2)
â”‚   â”‚   â”œâ”€â”€ global_query.py        # q_avg pooling (Eq.3)
â”‚   â”‚   â””â”€â”€ agreement.py           # dot-product gatta (Eq.4)
â”‚   â”‚
â”‚   â”œâ”€â”€ modulation/
â”‚   â”‚   â””â”€â”€ gain_control.py        # activation * (1 + Î±*gatta) (Eq.5)
â”‚   â”‚
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ resnet_wrapper.py      # Frozen ResNet + hooks on selected layers
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ gatta_cnn.py           # Backbone + global attention forward
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # d, selected layers, alpha init
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                 # GAttANet overview figure
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
